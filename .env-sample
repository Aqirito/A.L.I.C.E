MODEL_NAME_OR_PATH="Pygmalion-13B-SuperHOT-8K-GPTQ"   # Folder name of the model
TEMPLATE_TYPE="pygmalion"                   # for now is 'pygmalion' and 'prompt'
MODEL_TYPE="GPTQ"                           # GPTQ
MODEL_LOADER="ExLlama"                      # AutoGPTQ, HuggingFaceBig, ExLlama
LANGUAGE="[EN]"                             # [EN], [JA], [ZH], [KO]
SPEED=0.77                                  # good results is 0.77
SPEAKER_ID=607                              # speaker id

# Base model config
temperature=0.4
max_length=5120
max_new_tokens=5120
top_k=0                                     # consider the most probable top_k samples, 0 to disable top_k sampling
top_p=0.4                                   # consider tokens up to a cumulative probabiltiy of top_p, 0.0 to disable top_p sampling
typical_p=0.5
repetition_penalty=1.15
penalty_alpha=0.6
do_sample=true


# ExLlama Model Config extends from Base
min_p=0.2                                   # Do not consider tokens with probability less than this
typical=0.2                                   # Locally typical sampling threshold, 0.0 to disable typical sampling
token_repetition_penalty_max=1.2            # Repetition penalty for most recent tokens
token_repetition_penalty_sustain=256        # No. most recent tokens to repeat penalty for, -1 to apply to whole context
token_repetition_penalty_decay=128          # Gradually decrease penalty over this many tokens
beams=1
beam_length=1